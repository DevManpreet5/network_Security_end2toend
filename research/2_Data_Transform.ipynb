{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data=pd.read_csv('/Users/s/Desktop/mlops/network_Security_end2end/artifact/data/processed/train.csv')\n",
    "group_mapping = {\n",
    "    'BENIGN': 'Normal Traffic',\n",
    "    'DoS Hulk': 'DoS',\n",
    "    'DDoS': 'DDoS',\n",
    "    'PortScan': 'Port Scanning',\n",
    "    'DoS GoldenEye': 'DoS',\n",
    "    'FTP-Patator': 'Brute Force',\n",
    "    'DoS slowloris': 'DoS',\n",
    "    'DoS Slowhttptest': 'DoS',\n",
    "    'SSH-Patator': 'Brute Force',\n",
    "    'Bot': 'Bots',\n",
    "    'Web Attack � Brute Force': 'Web Attacks',\n",
    "    'Web Attack � XSS': 'Web Attacks',\n",
    "    'Infiltration': 'Infiltration',\n",
    "    'Web Attack � Sql Injection': 'Web Attacks',\n",
    "    'Heartbleed': 'Miscellaneous'\n",
    "}\n",
    "\n",
    "data['Attack Type'] = data['Label'].map(group_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attack Type\n",
       "Normal Traffic    1062672\n",
       "DoS                118118\n",
       "Port Scanning       74299\n",
       "DDoS                59853\n",
       "Brute Force          6468\n",
       "Web Attacks          1020\n",
       "Bots                  919\n",
       "Infiltration           17\n",
       "Miscellaneous           5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Attack Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns='Label', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.drop(columns=['Attack Type'])\n",
    "y=data['Attack Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attack Type\n",
       "6    1062672\n",
       "3     118118\n",
       "7      74299\n",
       "2      59853\n",
       "1       6468\n",
       "8       1020\n",
       "0        919\n",
       "4         17\n",
       "5          5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Attack Type'] = le.fit_transform(data['Attack Type'])\n",
    "data['Attack Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = pd.DataFrame(X).fillna(0)\n",
    "X = np.where(np.isinf(X), 0, X)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "X= imputer.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature  Importance\n",
      "42       42    0.054726\n",
      "41       41    0.053957\n",
      "39       39    0.052657\n",
      "54       54    0.045941\n",
      "0         0    0.045515\n",
      "63       63    0.040766\n",
      "52       52    0.037657\n",
      "10       10    0.037336\n",
      "4         4    0.034248\n",
      "13       13    0.033366\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = pd.DataFrame(X).fillna(0)\n",
    "\n",
    "X = np.where(np.isinf(X), 0, X)\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=pd.DataFrame(X).columns)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_scaled_df, y)\n",
    "\n",
    "feature_importances = pd.DataFrame({'Feature': X_scaled_df.columns, 'Importance': rf.feature_importances_})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importances.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.to_csv('i.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to keep:\n",
      "    Feature  Importance  Normalized_Importance\n",
      "42       42    0.054726               1.000000\n",
      "41       41    0.053957               0.985951\n",
      "39       39    0.052657               0.962195\n",
      "54       54    0.045941               0.839467\n",
      "0         0    0.045515               0.831691\n",
      "63       63    0.040766               0.744916\n",
      "52       52    0.037657               0.688101\n",
      "10       10    0.037336               0.682233\n",
      "4         4    0.034248               0.625806\n",
      "13       13    0.033366               0.609690\n",
      "6         6    0.028405               0.519047\n",
      "66       66    0.027320               0.499223\n",
      "5         5    0.026142               0.477686\n",
      "12       12    0.024007               0.438683\n",
      "8         8    0.021852               0.399301\n",
      "40       40    0.021654               0.395680\n",
      "65       65    0.021401               0.391061\n",
      "53       53    0.021069               0.384990\n",
      "55       55    0.019286               0.352418\n",
      "69       69    0.017068               0.311877\n",
      "67       67    0.016765               0.306353\n",
      "35       35    0.016687               0.304927\n",
      "34       34    0.016133               0.294800\n",
      "17       17    0.014742               0.269383\n",
      "62       62    0.013697               0.250292\n",
      "74       74    0.013653               0.249478\n",
      "2         2    0.013227               0.241695\n",
      "37       37    0.012974               0.237074\n",
      "14       14    0.012861               0.235007\n",
      "68       68    0.012607               0.230372\n",
      "46       46    0.012584               0.229945\n",
      "23       23    0.011954               0.218432\n",
      "1         1    0.010899               0.199148\n",
      "21       21    0.010622               0.194094\n",
      "47       47    0.010417               0.190352\n",
      "16       16    0.009753               0.178218\n",
      "24       24    0.009529               0.174120\n",
      "64       64    0.009514               0.173845\n",
      "22       22    0.009317               0.170240\n",
      "3         3    0.009208               0.168260\n",
      "15       15    0.009199               0.168094\n",
      "11       11    0.009162               0.167417\n",
      "9         9    0.007976               0.145752\n",
      "18       18    0.007868               0.143772\n",
      "36       36    0.007519               0.137400\n",
      "77       77    0.007208               0.131708\n",
      "20       20    0.006402               0.116975\n",
      "19       19    0.004815               0.087992\n",
      "76       76    0.004736               0.086545\n",
      "38       38    0.003720               0.067982\n",
      "7         7    0.002812               0.051385\n",
      "\n",
      "Features to remove:\n",
      "    Feature    Importance  Normalized_Importance\n",
      "48       48  2.093930e-03               0.038262\n",
      "25       25  2.066270e-03               0.037757\n",
      "72       72  1.984664e-03               0.036266\n",
      "29       29  1.567112e-03               0.028636\n",
      "28       28  1.563946e-03               0.028578\n",
      "70       70  1.258369e-03               0.022994\n",
      "26       26  1.196861e-03               0.021870\n",
      "27       27  1.170018e-03               0.021380\n",
      "73       73  1.042524e-03               0.019050\n",
      "43       43  8.136137e-04               0.014867\n",
      "51       51  6.157104e-04               0.011251\n",
      "30       30  6.150342e-04               0.011238\n",
      "71       71  4.278739e-04               0.007819\n",
      "44       44  3.600027e-04               0.006578\n",
      "75       75  2.653091e-04               0.004848\n",
      "32       32  1.390720e-05               0.000254\n",
      "49       49  1.017440e-05               0.000186\n",
      "45       45  2.868545e-07               0.000005\n",
      "61       61  0.000000e+00               0.000000\n",
      "60       60  0.000000e+00               0.000000\n",
      "59       59  0.000000e+00               0.000000\n",
      "58       58  0.000000e+00               0.000000\n",
      "57       57  0.000000e+00               0.000000\n",
      "56       56  0.000000e+00               0.000000\n",
      "31       31  0.000000e+00               0.000000\n",
      "33       33  0.000000e+00               0.000000\n",
      "50       50  0.000000e+00               0.000000\n"
     ]
    }
   ],
   "source": [
    "max_importance = feature_importances['Importance'].max()\n",
    "feature_importances['Normalized_Importance'] = feature_importances['Importance'] / max_importance\n",
    "\n",
    "threshold = 0.05  \n",
    "threshold_value = threshold * max_importance\n",
    "\n",
    "important_features = feature_importances[feature_importances['Importance'] >= threshold_value]\n",
    "\n",
    "features_to_remove = feature_importances[feature_importances['Importance'] < threshold_value]\n",
    "\n",
    "print(\"Features to keep:\")\n",
    "print(important_features)\n",
    "\n",
    "print(\"\\nFeatures to remove:\")\n",
    "print(features_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Normalize feature importance (optional)\n",
    "max_importance = feature_importances['Importance'].max()\n",
    "feature_importances['Normalized_Importance'] = feature_importances['Importance'] / max_importance\n",
    "\n",
    "# Loop to check multiple thresholds\n",
    "thresholds = np.linspace(0.05, 0.5, 10)  # Thresholds from 5% to 50%\n",
    "best_score = -np.inf\n",
    "best_threshold = None\n",
    "best_model = None\n",
    "best_features = None\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Determine the importance threshold\n",
    "    threshold_value = threshold * max_importance\n",
    "\n",
    "    # Select features above the threshold\n",
    "    selected_features = feature_importances[feature_importances['Importance'] >= threshold_value]['Feature']\n",
    "    \n",
    "    # Filter dataset to use only selected features\n",
    "    X_selected = X_scaled_df[selected_features]\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model with cross-validation (use accuracy or another metric as needed)\n",
    "    cv_score = np.mean(cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy'))\n",
    "    \n",
    "    # If the current model performs better, save it\n",
    "    if cv_score > best_score:\n",
    "        best_score = cv_score\n",
    "        best_threshold = threshold\n",
    "        best_model = model\n",
    "        best_features = selected_features\n",
    "\n",
    "    print(f\"Threshold: {threshold:.2f}, CV Score: {cv_score:.4f}\")\n",
    "\n",
    "# Output the best threshold and model performance\n",
    "print(f\"\\nBest Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Best CV Score: {best_score:.4f}\")\n",
    "print(f\"Selected Features: {best_features.tolist()}\")\n",
    "\n",
    "\n",
    "plt.plot(thresholds, best_score, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"CV Accuracy\")\n",
    "plt.title(\"Model Performance vs Feature Importance Threshold\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
